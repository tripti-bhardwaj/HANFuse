{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a46173",
   "metadata": {
    "id": "11a46173"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD  \n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ebde7",
   "metadata": {
    "id": "902ebde7"
   },
   "source": [
    "#### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3419d51c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3419d51c",
    "outputId": "6593db6f-a431-4df7-a66f-51d8c13a5755",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 6)  Dev: (20000, 6)  Test: (20000, 6)\n",
      "            id                                               text        user  \\\n",
      "51732  40223.0  @USER Ngl এটা দেখায় अच्छा किन्तु अनानास पेशाब...  jennifer55   \n",
      "22250      NaN  @USER @USER I thought তারা বলেছিল नंबर थे আপ? ...         NaN   \n",
      "19726      NaN  @USER He lets her ramble on for কিছুটা, হাসছে ...         NaN   \n",
      "12676  21487.0  @USER प्राप्त करें that shit আমার বন্ধ ফোন বোন...      matt31   \n",
      "40169      NaN  @USER @USER मैं खड़ा हूं corrected! Guess the ...         NaN   \n",
      "\n",
      "      state      time  label  \n",
      "51732    VA  20:53:45      0  \n",
      "22250   NaN       NaN      0  \n",
      "19726   NaN       NaN      0  \n",
      "12676    FL   5:53:51      1  \n",
      "40169   NaN       NaN      0               id                                               text    user  \\\n",
      "17297  76555.0  @USER I বলেছিলাম रोक लेना मेरा खा रहा है booty...   ann87   \n",
      "12994  12914.0  @USER এটা আসছে আমার প্রিয় প্রেসিডেন্ট ওবামা! ...  matt31   \n",
      "19412  94350.0  @USER @USER ওহ খুব handsome Archie that looks ...   ann87   \n",
      "12176  50852.0  .@USER announced that वह तैयार है প্রতিনিধিত্ব...   ann87   \n",
      "6671   11763.0  @USER आप नहीं कर सकते যত্ন করার ভান করা about ...  matt31   \n",
      "\n",
      "      state      time  label  \n",
      "17297    FL  11:33:46      1  \n",
      "12994    NY   1:59:39      0  \n",
      "19412    VA   0:20:50      0  \n",
      "12176    FL  16:25:31      0  \n",
      "6671     VA  18:48:54      0               id                                               text        user  \\\n",
      "10887  99844.0  @USER @USER ডোনাল্ড ট্রাম্প বলেছেন মিঃ ক্রুজ ম...       sam42   \n",
      "17666  29727.0  @USER যদি মিডিয়া রিপোর্ট করা বন্ধ করে দেয় তা...      bob747   \n",
      "18841  52432.0  @USER #Trump gifted the 1% with #GOPTaxScam सब...  jennifer55   \n",
      "13099  33037.0  @USER आप हैं একটি অবিশ্বাস্য ব্যক্তি pal. I kn...    george45   \n",
      "12801  18365.0  @USER Most men are decent और आदरणीय तो कैसा है...    george45   \n",
      "\n",
      "      state      time  label  \n",
      "10887    DE   0:27:59      1  \n",
      "17666    NY  12:39:18      1  \n",
      "18841    DE  19:11:47      1  \n",
      "13099    VA  15:01:42      0  \n",
      "12801    FL  12:25:28      1  \n"
     ]
    }
   ],
   "source": [
    "df_tr = pd.read_csv(\"/Users/triptibhardwaj/Downloads/train.csv\")\n",
    "df_va = pd.read_csv(\"/Users/triptibhardwaj/Downloads/dev.csv\")\n",
    "df_te = pd.read_csv(\"/Users/triptibhardwaj/Downloads/test.csv\")\n",
    "\n",
    "label_map = {\"OFF\": 1, \"NOT\": 0}\n",
    "df_tr[\"label\"] = df_tr[\"label\"].map(label_map)\n",
    "df_va[\"label\"] = df_va[\"label\"].map(label_map)\n",
    "df_te[\"label\"] = df_te[\"label\"].map(label_map)\n",
    "\n",
    "print(\"Train:\", df_tr.shape, \" Dev:\", df_va.shape, \" Test:\", df_te.shape)\n",
    "print(df_tr.sample(5), df_va.sample(5), df_te.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71c571",
   "metadata": {
    "id": "2d71c571"
   },
   "source": [
    "#### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39606f13",
   "metadata": {
    "id": "39606f13"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx].split()\n",
    "        ids = [self.vocab.get(tok, 1) for tok in tokens]\n",
    "        return {\n",
    "            \"doc_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"y_off\": torch.tensor(self.labels[idx], dtype=torch.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075156a",
   "metadata": {
    "id": "6075156a"
   },
   "source": [
    "#### HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84560821",
   "metadata": {
    "id": "84560821"
   },
   "outputs": [],
   "source": [
    "class WordAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn = nn.Linear(2*hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)  # [B, W, E]\n",
    "        h, _ = self.rnn(emb) # [B, W, 2H]\n",
    "        a = torch.softmax(self.attn(h), dim=1) # [B, W, 1]\n",
    "        rep = (a * h).sum(dim=1)               # [B, 2H]\n",
    "        return rep\n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=100, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.word_encoder = WordAttention(vocab_size, emb_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, doc_ids):\n",
    "        return self.word_encoder(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc94d3",
   "metadata": {
    "id": "53dc94d3"
   },
   "source": [
    "#### Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1af53c0",
   "metadata": {
    "id": "b1af53c0"
   },
   "outputs": [],
   "source": [
    "class FusionHANModel(nn.Module):\n",
    "    def __init__(self, vocab_size, tfidf_dim, emb_dim=100, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.han = HAN(vocab_size, emb_dim, hidden_dim)\n",
    "        self.fc_off = nn.Linear(2*hidden_dim + tfidf_dim, 1)\n",
    "\n",
    "    def forward(self, doc_ids, feats):\n",
    "        han_vec = self.han(doc_ids)\n",
    "        x = torch.cat([han_vec, feats], dim=1)\n",
    "        return {\"offense\": self.fc_off(x).squeeze(1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b254223",
   "metadata": {
    "id": "7b254223"
   },
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab76a4cf",
   "metadata": {
    "id": "ab76a4cf"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        self.device = device\n",
    "\n",
    "    def step_batch(self, batch, feats):\n",
    "        self.model.train()\n",
    "        self.opt.zero_grad()\n",
    "        out = self.model(batch[\"doc_ids\"].to(self.device), feats.to(self.device))\n",
    "        loss = F.binary_cross_entropy_with_logits(out[\"offense\"], batch[\"y_off\"].to(self.device))\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "    def evaluate(self, loader, feats):\n",
    "        self.model.eval()\n",
    "        ys, yh = [], []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                f = feats[i*batch_size:(i+1)*batch_size]\n",
    "                out = self.model(batch[\"doc_ids\"].to(self.device), f.to(self.device))\n",
    "                yh.extend(torch.sigmoid(out[\"offense\"]).cpu().numpy() > 0.5)\n",
    "                ys.extend(batch[\"y_off\"].numpy())\n",
    "        print(classification_report(ys, yh, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0672be7",
   "metadata": {
    "id": "e0672be7"
   },
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76ae6c3",
   "metadata": {
    "id": "b76ae6c3"
   },
   "outputs": [],
   "source": [
    "def train_model(trainer, train_loader, dev_loader, Xtr, Xva, num_epochs=30, batch_size=32):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            f = torch.tensor(Xtr[i*batch_size:(i+1)*batch_size], dtype=torch.float32)\n",
    "            trainer.step_batch(batch, f)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956441a",
   "metadata": {
    "id": "5956441a"
   },
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b964a9bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b964a9bf",
    "outputId": "dfde8c05-27d8-4d7f-d96f-8d9fd53d5d44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 completed.\n",
      "Epoch 2/30 completed.\n",
      "Epoch 3/30 completed.\n",
      "Epoch 4/30 completed.\n",
      "Epoch 5/30 completed.\n",
      "Epoch 6/30 completed.\n",
      "Epoch 7/30 completed.\n",
      "Epoch 8/30 completed.\n",
      "Epoch 9/30 completed.\n",
      "Epoch 10/30 completed.\n",
      "Epoch 11/30 completed.\n",
      "Epoch 12/30 completed.\n",
      "Epoch 13/30 completed.\n",
      "Epoch 14/30 completed.\n",
      "Epoch 15/30 completed.\n",
      "Epoch 16/30 completed.\n",
      "Epoch 17/30 completed.\n",
      "Epoch 18/30 completed.\n",
      "Epoch 19/30 completed.\n",
      "Epoch 20/30 completed.\n",
      "Epoch 21/30 completed.\n",
      "Epoch 22/30 completed.\n",
      "Epoch 23/30 completed.\n",
      "Epoch 24/30 completed.\n",
      "Epoch 25/30 completed.\n",
      "Epoch 26/30 completed.\n",
      "Epoch 27/30 completed.\n",
      "Epoch 28/30 completed.\n",
      "Epoch 29/30 completed.\n",
      "Epoch 30/30 completed.\n",
      "\n",
      "=== Final Test Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.911     0.969     0.939     13340\n",
      "         1.0      0.929     0.810     0.865      6660\n",
      "\n",
      "    accuracy                          0.916     20000\n",
      "   macro avg      0.920     0.889     0.902     20000\n",
      "weighted avg      0.917     0.916     0.914     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Vocabulary\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for sent in df_tr[\"text\"]:\n",
    "        for tok in sent.split():\n",
    "            if tok not in vocab:\n",
    "                vocab[tok] = len(vocab)\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "    tfidf.fit(df_tr[\"text\"])\n",
    "    Xtr = tfidf.transform(df_tr[\"text\"]).toarray()\n",
    "    Xva = tfidf.transform(df_va[\"text\"]).toarray()\n",
    "    Xte = tfidf.transform(df_te[\"text\"]).toarray()\n",
    "\n",
    "    # Datasets + Loaders\n",
    "    train_set = TextDataset(df_tr, vocab)\n",
    "    dev_set   = TextDataset(df_va, vocab)\n",
    "    test_set  = TextDataset(df_te, vocab)\n",
    "\n",
    "    global batch_size\n",
    "    batch_size = 32\n",
    "    tr_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=lambda x: {\n",
    "        \"doc_ids\": nn.utils.rnn.pad_sequence([item[\"doc_ids\"] for item in x], batch_first=True),\n",
    "        \"y_off\": torch.stack([item[\"y_off\"] for item in x])\n",
    "    })\n",
    "    va_loader = DataLoader(dev_set, batch_size=batch_size, shuffle=False, collate_fn=lambda x: {\n",
    "        \"doc_ids\": nn.utils.rnn.pad_sequence([item[\"doc_ids\"] for item in x], batch_first=True),\n",
    "        \"y_off\": torch.stack([item[\"y_off\"] for item in x])\n",
    "    })\n",
    "    te_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=lambda x: {\n",
    "        \"doc_ids\": nn.utils.rnn.pad_sequence([item[\"doc_ids\"] for item in x], batch_first=True),\n",
    "        \"y_off\": torch.stack([item[\"y_off\"] for item in x])\n",
    "    })\n",
    "\n",
    "    # Model + Trainer\n",
    "    model = FusionHANModel(len(vocab), tfidf_dim=5000)\n",
    "    trainer = Trainer(model, device=\"cpu\")\n",
    "\n",
    "    # Train\n",
    "    train_model(trainer, tr_loader, va_loader, Xtr, Xva, num_epochs=30, batch_size=batch_size)\n",
    "\n",
    "    # Final Test\n",
    "    print(\"\\n=== Final Test Evaluation ===\")\n",
    "    trainer.evaluate(te_loader, torch.tensor(Xte, dtype=torch.float32))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
