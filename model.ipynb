{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a46173",
   "metadata": {
    "id": "11a46173"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD  \n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ebde7",
   "metadata": {
    "id": "902ebde7"
   },
   "source": [
    "#### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3419d51c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3419d51c",
    "outputId": "6593db6f-a431-4df7-a66f-51d8c13a5755",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 6)  Dev: (20000, 6)  Test: (20000, 6)\n",
      "            id                                               text        user  \\\n",
      "37708  45353.0  @USER @USER @USER @USER à¦¸à¦¾à¦¦à¦¾ à¦‰à¦¦à¦¾à¦°à¦ªà¦¨à§à¦¥à§€ do not ...       ann87   \n",
      "11367  90011.0                        @USER à¦­à¦¾à¦‡...à¦¸à§‡ à¤¹à¥ˆ cray mehn  jennifer55   \n",
      "7410       NaN  @USER @USER @USER @USER @USER @USER à¤¯à¤¹ à¤à¤‚à¤Ÿà¥€à¤«à¤¾ ...         NaN   \n",
      "26700  27255.0             @USER à¤•à¤° à¤¸à¤•à¤¨à¤¾ à¦†à¦ªà¦¨à¦¿ say DESPERATE?!?!?!       sam42   \n",
      "57749  52628.0     @USER Iâ€™ll à¦¯à¦¾à¦“à¦¯à¦¼à¦¾ à¤šà¥à¤¨à¤¨à¤¾ u up à¤²à¤¾à¤¨à¤¤ à¤¹à¥ˆ à¦à¦•à¦Ÿà¦¿ à¤‰à¤¬à¥‡à¤°     steve11   \n",
      "\n",
      "      state      time  label  \n",
      "37708    CT  12:09:07      0  \n",
      "11367    FL   1:39:29      0  \n",
      "7410    NaN       NaN      1  \n",
      "26700    NY   0:22:52      0  \n",
      "57749    VA  17:57:49      1               id                                               text        user  \\\n",
      "12041  16051.0                       @USER Exactly... à¤µà¥‡ à¦¸à¦¬ à¤•à¤¡à¤¼à¤µà¤¾  jennifer55   \n",
      "18558      NaN  .@USER à¤¹à¥ˆ her own à¦¬à§à¦¯à¦•à§à¦¤à¦¿ à¦¸à§‡ à¦à¦Ÿà¦¿ à¦¨à¦¾ a @USER à¤¨à¤•...         NaN   \n",
      "8492   49300.0  @USER Well your a à¤‰à¤¦à¤¾à¤°à¤µà¤¾à¤¦à¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤ªà¤•à¤°à¤£ à¦¤à¦¾à¦‡...    george45   \n",
      "12707  92046.0  @USER @USER à¤¹à¤¾à¤‚ à¤®à¥‚à¤² à¤°à¥‚à¤ª à¤¸à¥‡ another à¦®à§ƒà¦¤ à¦¬à¦¿à¦¡à¦¼à¦¾à¦² URL      matt31   \n",
      "17925  49189.0  @USER @USER Agree 100% Jay No wonder FBI à¤¬à¥ˆà¤•à¤¬à¤°...    george45   \n",
      "\n",
      "      state      time  label  \n",
      "12041    DE  14:50:13      1  \n",
      "18558   NaN       NaN      0  \n",
      "8492     FL  23:09:51      0  \n",
      "12707    CT   2:47:24      0  \n",
      "17925    NY   0:58:38      0               id                                               text        user  \\\n",
      "17270  54521.0  @USER à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¦à§‡à¦¶ à¦•à¦¿ à¦ªà¦¾à¦°à¦¬à§‡? à¤…à¤ªà¤¨à¥‡ à¤•à¤¾à¤¨à¥‚à¤¨ à¤ªà¥à¤°à¤µà¤°à¥à¤¤à¤¨...      matt31   \n",
      "5301   75579.0                         @USER à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¦¸à§‡à¦•à§à¦¸à¦¿â¤ðŸ˜ðŸ‘Œ       ann87   \n",
      "145    70822.0                                @USER à¤µà¤¹ is à¤à¤• à¦¦à¦¾à¦¨à¦¬      matt31   \n",
      "735    56141.0  @USER à¤¨à¤¹à¥€à¤‚ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ seem like à¦¹à¦²à¦¿à¦‰à¦¡à§‡ à¦†à¦›à§‡ à¦…à¦¨à§‡à¦• à¦¬...  jennifer55   \n",
      "1769   97312.0  à¦¯à¦¦à¦¿ à¦†à¦®à¦¾à¦•à§‡ à¦•à¦°à¦¤à§‡ à¦¹à¦¯à¦¼ tolerate ur loud ass à¤®à¥ˆà¤—à¤¾ à¤®...  jennifer55   \n",
      "\n",
      "      state      time  label  \n",
      "17270    VA   7:19:36      0  \n",
      "5301     DE  23:03:42      0  \n",
      "145      NY  19:38:24      1  \n",
      "735      FL  15:05:54      0  \n",
      "1769     DE   5:38:17      1  \n"
     ]
    }
   ],
   "source": [
    "df_tr = pd.read_csv(\"/Users/triptibhardwaj/Downloads/train.csv\")\n",
    "df_va = pd.read_csv(\"/Users/triptibhardwaj/Downloads/dev.csv\")\n",
    "df_te = pd.read_csv(\"/Users/triptibhardwaj/Downloads/test.csv\")\n",
    "\n",
    "label_map = {\"OFF\": 1, \"NOT\": 0}\n",
    "df_tr[\"label\"] = df_tr[\"label\"].map(label_map)\n",
    "df_va[\"label\"] = df_va[\"label\"].map(label_map)\n",
    "df_te[\"label\"] = df_te[\"label\"].map(label_map)\n",
    "\n",
    "print(\"Train:\", df_tr.shape, \" Dev:\", df_va.shape, \" Test:\", df_te.shape)\n",
    "print(df_tr.sample(5), df_va.sample(5), df_te.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71c571",
   "metadata": {
    "id": "2d71c571"
   },
   "source": [
    "#### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39606f13",
   "metadata": {
    "id": "39606f13"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx].split()\n",
    "        ids = [self.vocab.get(tok, 1) for tok in tokens]  # unk=1\n",
    "        return {\n",
    "            \"doc_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"y_off\": torch.tensor(self.labels[idx], dtype=torch.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075156a",
   "metadata": {
    "id": "6075156a"
   },
   "source": [
    "#### HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84560821",
   "metadata": {
    "id": "84560821"
   },
   "outputs": [],
   "source": [
    "class WordAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn = nn.Linear(2*hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)  # [B, W, E]\n",
    "        h, _ = self.rnn(emb) # [B, W, 2H]\n",
    "        a = torch.softmax(self.attn(h), dim=1) # [B, W, 1]\n",
    "        rep = (a * h).sum(dim=1)               # [B, 2H]\n",
    "        return rep\n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=100, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.word_encoder = WordAttention(vocab_size, emb_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, doc_ids):\n",
    "        return self.word_encoder(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc94d3",
   "metadata": {
    "id": "53dc94d3"
   },
   "source": [
    "#### Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1af53c0",
   "metadata": {
    "id": "b1af53c0"
   },
   "outputs": [],
   "source": [
    "class FusionHANModel(nn.Module):\n",
    "    def __init__(self, vocab_size, tfidf_dim, emb_dim=100, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.han = HAN(vocab_size, emb_dim, hidden_dim)\n",
    "        self.fc_off = nn.Linear(2*hidden_dim + tfidf_dim, 1)\n",
    "\n",
    "    def forward(self, doc_ids, feats):\n",
    "        han_vec = self.han(doc_ids)\n",
    "        x = torch.cat([han_vec, feats], dim=1)\n",
    "        return {\"offense\": self.fc_off(x).squeeze(1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b254223",
   "metadata": {
    "id": "7b254223"
   },
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab76a4cf",
   "metadata": {
    "id": "ab76a4cf"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        self.device = device\n",
    "\n",
    "    def step_batch(self, batch, feats):\n",
    "        self.model.train()\n",
    "        self.opt.zero_grad()\n",
    "        out = self.model(batch[\"doc_ids\"].to(self.device), feats.to(self.device))\n",
    "        loss = F.binary_cross_entropy_with_logits(out[\"offense\"], batch[\"y_off\"].to(self.device))\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "    def evaluate(self, loader, feats):\n",
    "        self.model.eval()\n",
    "        ys, yh = [], []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                f = feats[i*batch_size:(i+1)*batch_size]\n",
    "                out = self.model(batch[\"doc_ids\"].to(self.device), f.to(self.device))\n",
    "                yh.extend(torch.sigmoid(out[\"offense\"]).cpu().numpy() > 0.5)\n",
    "                ys.extend(batch[\"y_off\"].numpy())\n",
    "        print(classification_report(ys, yh, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0672be7",
   "metadata": {
    "id": "e0672be7"
   },
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76ae6c3",
   "metadata": {
    "id": "b76ae6c3"
   },
   "outputs": [],
   "source": [
    "def train_model(trainer, train_loader, dev_loader, Xtr, Xva, num_epochs=10, batch_size=32):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            f = torch.tensor(Xtr[i*batch_size:(i+1)*batch_size], dtype=torch.float32)\n",
    "            trainer.step_batch(batch, f)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956441a",
   "metadata": {
    "id": "5956441a"
   },
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b964a9bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b964a9bf",
    "outputId": "dfde8c05-27d8-4d7f-d96f-8d9fd53d5d44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed.\n",
      "Epoch 2/10 completed.\n",
      "Epoch 3/10 completed.\n",
      "Epoch 4/10 completed.\n",
      "Epoch 5/10 completed.\n",
      "Epoch 6/10 completed.\n",
      "Epoch 7/10 completed.\n",
      "Epoch 8/10 completed.\n",
      "Epoch 9/10 completed.\n",
      "Epoch 10/10 completed.\n",
      "\n",
      "=== Final Test Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.939     0.929     0.934     13340\n",
      "         1.0      0.861     0.878     0.869      6660\n",
      "\n",
      "    accuracy                          0.912     20000\n",
      "   macro avg      0.900     0.904     0.902     20000\n",
      "weighted avg      0.913     0.912     0.912     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Vocabulary\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for sent in df_tr[\"text\"]:\n",
    "        for tok in sent.split():\n",
    "            if tok not in vocab:\n",
    "                vocab[tok] = len(vocab)\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "    tfidf.fit(df_tr[\"text\"])\n",
    "    Xtr = tfidf.transform(df_tr[\"text\"]).toarray()\n",
    "    Xva = tfidf.transform(df_va[\"text\"]).toarray()\n",
    "    Xte = tfidf.transform(df_te[\"text\"]).toarray()\n",
    "\n",
    "    # Datasets + Loaders\n",
    "    train_set = TextDataset(df_tr, vocab)\n",
    "    dev_set   = TextDataset(df_va, vocab)\n",
    "    test_set  = TextDataset(df_te, vocab)\n",
    "\n",
    "    global batch_size\n",
    "    batch_size = 32\n",
    "    tr_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=lambda x: {\n",
    "        \"doc_ids\": nn.utils.rnn.pad_sequence([item[\"doc_ids\"] for item in x], batch_first=True),\n",
    "        \"y_off\": torch.stack([item[\"y_off\"] for item in x])\n",
    "    })\n",
    "    va_loader = DataLoader(dev_set, batch_size=batch_size, shuffle=False, collate_fn=lambda x: {\n",
    "        \"doc_ids\": nn.utils.rnn.pad_sequence([item[\"doc_ids\"] for item in x], batch_first=True),\n",
    "        \"y_off\": torch.stack([item[\"y_off\"] for item in x])\n",
    "    })\n",
    "    te_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=lambda x: {\n",
    "        \"doc_ids\": nn.utils.rnn.pad_sequence([item[\"doc_ids\"] for item in x], batch_first=True),\n",
    "        \"y_off\": torch.stack([item[\"y_off\"] for item in x])\n",
    "    })\n",
    "\n",
    "    # Model + Trainer\n",
    "    model = FusionHANModel(len(vocab), tfidf_dim=5000)\n",
    "    trainer = Trainer(model, device=\"cpu\")\n",
    "\n",
    "    # Train\n",
    "    train_model(trainer, tr_loader, va_loader, Xtr, Xva, num_epochs=10, batch_size=batch_size)\n",
    "\n",
    "    # Final Test\n",
    "    print(\"\\n=== Final Test Evaluation ===\")\n",
    "    trainer.evaluate(te_loader, torch.tensor(Xte, dtype=torch.float32))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
